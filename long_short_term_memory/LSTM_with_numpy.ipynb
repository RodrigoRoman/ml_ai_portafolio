{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigoRoman/ml_ai_portafolio/blob/main/long_short_term_memory/LSTM_with_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>LSTM</h1>"
      ],
      "metadata": {
        "id": "zaDZBz4WTDJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sHmc2S55S-Jb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "NRhyR1vhT_XB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_s_path = \"/content/drive/MyDrive/newsSpace\""
      ],
      "metadata": {
        "id": "iqPbkI3QUBFI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Data preprocessing functions</h2>"
      ],
      "metadata": {
        "id": "16ttythEUDAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def is_url(s):\n",
        "    # A simple regex to check for a basic URL structure\n",
        "    return re.match(r'https?://', s) is not None\n",
        "def tokenize_article(line):\n",
        "  url_index = next((i for i, item in enumerate(line) if is_url(item)), None)\n",
        "  if url_index is not None:\n",
        "    return re.split(r'[ ,.;:!?()]+', ' '.join(line[url_index+1:]))\n",
        "  return None\n",
        "\n",
        "\n",
        "def process_file(filepath, num_articles):\n",
        "  articles = []\n",
        "  vocabulary = set()\n",
        "  pattern = re.compile(r'[ ,.;:!?()]+')\n",
        "  word_pattern = re.compile(r\"\\b[A-Za-z]+'?[A-Za-z]*(?=\\s|\\b)\")\n",
        "  try:\n",
        "    with open(filepath, encoding='ISO-8859-1') as file:\n",
        "      data = file.read()\n",
        "      pattern = re.compile(r\"\\((Reuters|AP)\\)[\\t\\n]+(.*?)[\\t\\n]+\\d+[\\t\\n]+[0-9]{4}-[0-9]{2}-[0-9]{2}\", re.DOTALL)\n",
        "      raw_articles = pattern.findall(data)\n",
        "      print(\"amount of articles\")\n",
        "      print(len(raw_articles))\n",
        "\n",
        "      for article in raw_articles:\n",
        "        if len(articles) < num_articles:\n",
        "          article_text = article[1].strip()\n",
        "          # Cleaning and processing the article text\n",
        "          words = word_pattern.findall(article_text.lower())\n",
        "          # cleaned_article = ' '.join(words)\n",
        "          articles.append(words)\n",
        "          # Update vocabulary\n",
        "          vocabulary.update(words)\n",
        "  except IOError:\n",
        "    print(\"Error opening or reading the file\")\n",
        "    return [], set()\n",
        "  return articles, vocabulary"
      ],
      "metadata": {
        "id": "Kmr2qknBUHM2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qhzkV5WXjL8",
        "outputId": "e7a580ad-901c-4c70-e540-217cf2b7073d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>DataLoading</h2>"
      ],
      "metadata": {
        "id": "n1lf36QdUNKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_articles = 10\n",
        "news_s_path = \"/content/drive/MyDrive/newsSpace\"\n",
        "\n",
        "data_articles, vocabulary = process_file(news_s_path, num_articles)\n",
        "print(data_articles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--TCPcDoUQMB",
        "outputId": "81b9aba4-1377-410e-f2f4-d9ec4d8682cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amount of articles\n",
            "57469\n",
            "[['none', 'business', 'reuters', 'wall', \"street's\", 'long', 'playing', 'drama', 'waiting', 'for', 'google', 'is', 'about', 'to', 'reach', 'its', 'final', 'act', 'but', 'its', 'stock', 'market', 'debut', 'is', 'ending', 'up', 'as', 'more', 'of', 'a', 'nostalgia', 'event', 'than', 'the', 'catalyst', 'for', 'a', 'new', 'era'], ['none', 'business', 'reuters', 'short', 'sellers', 'wall', \"street's\", 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again'], ['none', 'business', 'reuters', 'private', 'investment', 'firm', 'carlyle', 'group', 'which', 'has', 'a', 'reputation', 'for', 'making', 'well', 'timed', 'and', 'occasionally', 'controversial', 'plays', 'in', 'the', 'defense', 'industry', 'has', 'quietly', 'placed', 'its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market'], ['none', 'business', 'reuters', 'soaring', 'crude', 'prices', 'plus', 'worries', 'about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to', 'hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the', 'summer', 'doldrums'], ['none', 'business', 'reuters', 'authorities', 'have', 'halted', 'oil', 'export', 'flows', 'from', 'the', 'main', 'pipeline', 'in', 'southern', 'iraq', 'after', 'intelligence', 'showed', 'a', 'rebel', 'militia', 'could', 'strike', 'infrastructure', 'an', 'oil', 'official', 'said', 'on', 'saturday'], ['none', 'business', 'reuters', 'stocks', 'ended', 'slightly', 'higher', 'on', 'friday', 'but', 'stayed', 'near', 'lows', 'for', 'the', 'year', 'as', 'oil', 'prices', 'surged', 'past', 'a', 'barrel', 'offsetting', 'a', 'positive', 'outlook', 'from', 'computer', 'maker', 'dell', 'inc', 'dell', 'o'], ['none', 'business', 'ap', 'assets', 'of', 'the', \"nation's\", 'retail', 'money', 'market', 'mutual', 'funds', 'fell', 'by', 'billion', 'in', 'the', 'latest', 'week', 'to', 'trillion', 'the', 'investment', 'company', 'institute', 'said', 'thursday'], ['none', 'sci', 'tech', 'reuters', 'was', 'absenteeism', 'a', 'little', 'high', 'on', 'tuesday', 'among', 'the', 'guys', 'at', 'the', 'office', 'ea', 'sports', 'would', 'like', 'to', 'think', 'it', 'was', 'because', 'madden', 'nfl', 'came', 'out', 'that', 'day', 'and', 'some', 'fans', 'of', 'the', 'football', 'simulation', 'are', 'rabid', 'enough', 'to', 'take', 'a', 'sick', 'day', 'to', 'play', 'it'], ['none', 'sci', 'tech', 'reuters', 'a', 'group', 'of', 'technology', 'companies', 'including', 'texas', 'instruments', 'inc', 'txn', 'n', 'stmicroelectronics', 'stm', 'pa', 'and', 'broadcom', 'corp', 'brcm', 'o', 'on', 'thursday', 'said', 'they', 'will', 'propose', 'a', 'new', 'wireless', 'networking', 'standard', 'up', 'to', 'times', 'the', 'speed', 'of', 'the', 'current', 'generation'], ['none', 'sci', 'tech', 'reuters', 'america', 'online', 'on', 'thursday', 'said', 'it', 'plans', 'to', 'sell', 'a', 'low', 'priced', 'pc', 'targeting', 'low', 'income', 'and', 'minority', 'households', 'who', 'agree', 'to', 'sign', 'up', 'for', 'a', 'year', 'of', 'dialup', 'internet', 'service']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Split data into train and test</h3>\n",
        "<p>Split data into training and testing sets, and further into input and target pairs where the target is the next word</p>"
      ],
      "metadata": {
        "id": "CxNakBZrUSzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess articles to create input-target pairs\n",
        "def create_input_target(articles):\n",
        "  article_targets = []\n",
        "  for article in articles:\n",
        "    target_article = []\n",
        "    for i in range(len(article) - 1):\n",
        "      target_word = article[i + 1]\n",
        "      target_article.append(target_word)\n",
        "    article_targets.append(target_article)\n",
        "  return article_targets\n",
        "\n",
        "# Split data into training and test sets\n",
        "def split_data(data, test_percentage):\n",
        "  split_point = int(len(data) * test_percentage)\n",
        "  test_set = data[:split_point]\n",
        "  training_set = data[split_point:]\n",
        "  return training_set, test_set\n",
        "\n",
        "\n",
        "# Example usage\n",
        "test_percentage = 0.2\n",
        "\n",
        "# Create input-target pairs\n",
        "article_targets  = create_input_target(data_articles)\n",
        "print(\"Input target pairs\")\n",
        "print(article_targets)\n",
        "\n",
        "# Vocabulary\n",
        "word_to_idx = {ch:i for (i,ch) in enumerate(list(vocabulary))}\n",
        "idx_to_word = {i:ch for (i,ch) in enumerate(list(vocabulary))}\n",
        "\n",
        "# Take input-target as x and y\n",
        "x_train_words, x_test_words = split_data(data_articles,test_percentage)\n",
        "y_train_words, y_test_words = split_data(article_targets,test_percentage)\n",
        "\n",
        "# Change the data to their index versions\n",
        "x_train = [[word_to_idx[word] for word in article if word in word_to_idx] for article in x_train_words]\n",
        "y_train = [[word_to_idx[word] for word in article if word in word_to_idx] for article in y_train_words]\n",
        "x_test = [[word_to_idx[word] for word in article if word in word_to_idx] for article in x_test_words]\n",
        "y_test = [[word_to_idx[word] for word in article if word in word_to_idx] for article in y_test_words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSwHMt0UVsx",
        "outputId": "81915902-e35e-45b3-d837-ce34b04e25e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input target pairs\n",
            "[['business', 'reuters', 'wall', \"street's\", 'long', 'playing', 'drama', 'waiting', 'for', 'google', 'is', 'about', 'to', 'reach', 'its', 'final', 'act', 'but', 'its', 'stock', 'market', 'debut', 'is', 'ending', 'up', 'as', 'more', 'of', 'a', 'nostalgia', 'event', 'than', 'the', 'catalyst', 'for', 'a', 'new', 'era'], ['business', 'reuters', 'short', 'sellers', 'wall', \"street's\", 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again'], ['business', 'reuters', 'private', 'investment', 'firm', 'carlyle', 'group', 'which', 'has', 'a', 'reputation', 'for', 'making', 'well', 'timed', 'and', 'occasionally', 'controversial', 'plays', 'in', 'the', 'defense', 'industry', 'has', 'quietly', 'placed', 'its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market'], ['business', 'reuters', 'soaring', 'crude', 'prices', 'plus', 'worries', 'about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to', 'hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the', 'summer', 'doldrums'], ['business', 'reuters', 'authorities', 'have', 'halted', 'oil', 'export', 'flows', 'from', 'the', 'main', 'pipeline', 'in', 'southern', 'iraq', 'after', 'intelligence', 'showed', 'a', 'rebel', 'militia', 'could', 'strike', 'infrastructure', 'an', 'oil', 'official', 'said', 'on', 'saturday'], ['business', 'reuters', 'stocks', 'ended', 'slightly', 'higher', 'on', 'friday', 'but', 'stayed', 'near', 'lows', 'for', 'the', 'year', 'as', 'oil', 'prices', 'surged', 'past', 'a', 'barrel', 'offsetting', 'a', 'positive', 'outlook', 'from', 'computer', 'maker', 'dell', 'inc', 'dell', 'o'], ['business', 'ap', 'assets', 'of', 'the', \"nation's\", 'retail', 'money', 'market', 'mutual', 'funds', 'fell', 'by', 'billion', 'in', 'the', 'latest', 'week', 'to', 'trillion', 'the', 'investment', 'company', 'institute', 'said', 'thursday'], ['sci', 'tech', 'reuters', 'was', 'absenteeism', 'a', 'little', 'high', 'on', 'tuesday', 'among', 'the', 'guys', 'at', 'the', 'office', 'ea', 'sports', 'would', 'like', 'to', 'think', 'it', 'was', 'because', 'madden', 'nfl', 'came', 'out', 'that', 'day', 'and', 'some', 'fans', 'of', 'the', 'football', 'simulation', 'are', 'rabid', 'enough', 'to', 'take', 'a', 'sick', 'day', 'to', 'play', 'it'], ['sci', 'tech', 'reuters', 'a', 'group', 'of', 'technology', 'companies', 'including', 'texas', 'instruments', 'inc', 'txn', 'n', 'stmicroelectronics', 'stm', 'pa', 'and', 'broadcom', 'corp', 'brcm', 'o', 'on', 'thursday', 'said', 'they', 'will', 'propose', 'a', 'new', 'wireless', 'networking', 'standard', 'up', 'to', 'times', 'the', 'speed', 'of', 'the', 'current', 'generation'], ['sci', 'tech', 'reuters', 'america', 'online', 'on', 'thursday', 'said', 'it', 'plans', 'to', 'sell', 'a', 'low', 'priced', 'pc', 'targeting', 'low', 'income', 'and', 'minority', 'households', 'who', 'agree', 'to', 'sign', 'up', 'for', 'a', 'year', 'of', 'dialup', 'internet', 'service']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>LSTM</h2>"
      ],
      "metadata": {
        "id": "iEmyBGBEUgYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    x_safe = x + 1e-12\n",
        "    f = 1 / (1 + np.exp(-x_safe))\n",
        "    return f * (1 - f)\n",
        "\n",
        "class LSTM:\n",
        "    def xavier_init(self,fan_in, fan_out):\n",
        "      limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "      return np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
        "\n",
        "    def __init__(self, hidden_size, vocab_size, learning_rate):\n",
        "      self.hidden_size = hidden_size\n",
        "      self.vocab_size = vocab_size\n",
        "      self.learning_rate = learning_rate\n",
        "      # Initialize LSTM parameters\n",
        "      input_size = hidden_size + vocab_size\n",
        "\n",
        "      #Forget gate\n",
        "      self.Wf = self.xavier_init(hidden_size, input_size)\n",
        "      self.Wi = self.xavier_init(hidden_size, input_size)\n",
        "      self.Wo = self.xavier_init(hidden_size, input_size)\n",
        "      self.Wc = self.xavier_init(hidden_size, input_size)\n",
        "      self.Wy = self.xavier_init(vocab_size, hidden_size)\n",
        "      self.bf = np.zeros((hidden_size, 1))\n",
        "      self.bi = np.zeros((hidden_size, 1))\n",
        "      self.bo = np.zeros((hidden_size, 1))\n",
        "      self.bc = np.zeros((hidden_size, 1))\n",
        "      self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    def softmax(self, x):\n",
        "      e_x = np.exp(x - np.max(x))\n",
        "      return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def cross_entropy(self, probs, targets):\n",
        "      loss = 0\n",
        "      epsilon = 1e-9\n",
        "      for t in range(len(targets)):\n",
        "        prob = max(probs[t][targets[t]], epsilon)\n",
        "        loss += -np.log(prob)\n",
        "      return loss\n",
        "    def forward(self, inputs, h_prev, c_prev):\n",
        "      caches = []\n",
        "      h_next, c_next = h_prev, c_prev\n",
        "      for t in range(len(inputs)):\n",
        "        z = np.row_stack((h_next, inputs[t]))\n",
        "        forget_gate = sigmoid(np.dot(self.Wf, z) + self.bf)\n",
        "        input_gate = sigmoid(np.dot(self.Wi, z) + self.bi)\n",
        "        output_gate = sigmoid(np.dot(self.Wo, z) + self.bo)\n",
        "        c_bar = np.tanh(np.dot(self.Wc, z) + self.bc)\n",
        "        c_next = forget_gate * c_next + input_gate * c_bar\n",
        "        h_next = output_gate * np.tanh(c_next)\n",
        "        y = np.dot(self.Wy, h_next) + self.by\n",
        "        p = self.softmax(y)\n",
        "        caches.append((h_next, c_next, forget_gate, input_gate, output_gate, c_bar, z, y, p,c_prev))\n",
        "      return caches\n",
        "\n",
        "    def backward(self, caches, targets):\n",
        "      # Initialize gradients\n",
        "      dWf, dWi, dWo, dWc, dWy = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wo), np.zeros_like(self.Wc), np.zeros_like(self.Wy)\n",
        "      dbf, dbi, dbo, dbc, dby = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bo), np.zeros_like(self.bc), np.zeros_like(self.by)\n",
        "      dh_next, dc_next = np.zeros_like(caches[0][0]), np.zeros_like(caches[0][1])\n",
        "      # Backpropagation through time\n",
        "      for t in reversed(range(len(targets))):\n",
        "        dh_next, dc_next, dWf_t, dWi_t, dWo_t, dWc_t, dbf_t, dbi_t, dbo_t, dbc_t, dWy_t, dby_t = self.lstm_step_backward(dh_next, dc_next, caches[t], targets[t])\n",
        "        dWf += dWf_t\n",
        "        dWi += dWi_t\n",
        "        dWo += dWo_t\n",
        "        dWc += dWc_t\n",
        "        dbf += dbf_t\n",
        "        dbi += dbi_t\n",
        "        dbo += dbo_t\n",
        "        dbc += dbc_t\n",
        "        dWy += dWy_t\n",
        "        dby += dby_t\n",
        "      return dWf, dWi, dWo, dWc, dWy, dbf, dbi, dbo, dbc, dby\n",
        "\n",
        "    def lstm_step_backward(self, dh_next, dc_next, cache, target):\n",
        "        # Unpack cache\n",
        "        h_next, c_next, f, i, o, c_bar, z, y, p,c_prev = cache\n",
        "\n",
        "        # Gradients of loss with respect to y\n",
        "        dy = np.copy(p)\n",
        "        dy[target] -= 1\n",
        "\n",
        "        # Gradients with respect to Wy and by\n",
        "        dWy = np.dot(dy, h_next.T)\n",
        "        dby = dy\n",
        "\n",
        "        # Gradients with respect to h_next\n",
        "        dh = np.dot(self.Wy.T, dy) + dh_next\n",
        "\n",
        "        # Gradients with respect to o\n",
        "        do = dh * np.tanh(c_next)\n",
        "        do = do * o * (1 - o)\n",
        "\n",
        "        # Gradients with respect to c_next\n",
        "        dc = dh * o * (1 - np.tanh(c_next)**2) + dc_next\n",
        "\n",
        "        # Gradients with respect to i\n",
        "        di = dc * c_bar\n",
        "        di = di * i * (1 - i)\n",
        "\n",
        "        # Gradients with respect to c_bar\n",
        "        dc_bar = dc * i\n",
        "        dc_bar = dc_bar * (1 - c_bar**2)\n",
        "\n",
        "        # Gradients with respect to f\n",
        "        df = dc * c_prev\n",
        "        df = df * f * (1 - f)\n",
        "\n",
        "        # Gradients with respect to z\n",
        "        dz = (np.dot(self.Wf.T, df)\n",
        "              + np.dot(self.Wi.T, di)\n",
        "              + np.dot(self.Wo.T, do)\n",
        "              + np.dot(self.Wc.T, dc_bar))\n",
        "\n",
        "        # Gradients with respect to the weights and biases\n",
        "        dWf = np.dot(df, z.T)\n",
        "        dWi = np.dot(di, z.T)\n",
        "        dWo = np.dot(do, z.T)\n",
        "        dWc = np.dot(dc_bar, z.T)\n",
        "        dbf = df\n",
        "        dbi = di\n",
        "        dbo = do\n",
        "        dbc = dc_bar\n",
        "\n",
        "        # Compute gradients with respect to the previous hidden state and cell state\n",
        "        dh_prev = dz[:self.hidden_size, :]\n",
        "        dc_prev = f * dc\n",
        "\n",
        "        return dh_prev, dc_prev, dWf, dWi, dWo, dWc, dbf, dbi, dbo, dbc, dWy, dby\n",
        "\n",
        "    def update_parameters(self, dWf, dWi, dWo, dWc, dWy, dbf, dbi, dbo, dbc, dby):\n",
        "        # Update the weights and biases using the gradients and the learning rate\n",
        "        self.Wf -= self.learning_rate * dWf\n",
        "        self.Wi -= self.learning_rate * dWi\n",
        "        self.Wo -= self.learning_rate * dWo\n",
        "        self.Wc -= self.learning_rate * dWc\n",
        "        self.Wy -= self.learning_rate * dWy\n",
        "        self.bf -= self.learning_rate * dbf\n",
        "        self.bi -= self.learning_rate * dbi\n",
        "        self.bo -= self.learning_rate * dbo\n",
        "        self.bc -= self.learning_rate * dbc\n",
        "        self.by -= self.learning_rate * dby\n",
        "\n",
        "    def train(self, x_train, y_train, epochs):\n",
        "      print(\"inside train\")\n",
        "      print(len(x_train))\n",
        "      for epoch in range(epochs):\n",
        "        print(\"Inside epoch\")\n",
        "        h_prev = np.zeros((self.hidden_size, 1))\n",
        "        c_prev = np.zeros((self.hidden_size, 1))\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        rand_print = random.randint(1, len(x_train)-1)\n",
        "\n",
        "        for batch_idx, inputs in enumerate(x_train):\n",
        "          print(\"inside batch\")\n",
        "          # Convert inputs to one-hot encoded vectors\n",
        "          inputs_one_hot = [np.eye(self.vocab_size)[:, x].reshape(-1, 1) for x in inputs]\n",
        "\n",
        "          # Forward pass\n",
        "          caches = self.forward(inputs_one_hot, h_prev, c_prev)\n",
        "\n",
        "          # Backward pass\n",
        "          dWf, dWi, dWo, dWc, dWy, dbf, dbi, dbo, dbc, dby = self.backward(caches, y_train[batch_idx])\n",
        "\n",
        "          # Update parameters\n",
        "          self.update_parameters(dWf, dWi, dWo, dWc, dWy, dbf, dbi, dbo, dbc, dby)\n",
        "\n",
        "          # Update the hidden and cell states\n",
        "          h_prev, c_prev = caches[-1][0], caches[-1][1]\n",
        "\n",
        "          # Compute predictions and accuracy\n",
        "          predictions = [min(np.argmax(cache[-1]), len(vocabulary) - 1) for cache in caches]\n",
        "          targets = y_train[batch_idx]\n",
        "          predictions = predictions[:len(targets)]\n",
        "          for t, prediction in enumerate(predictions):\n",
        "            correct_predictions += (prediction == targets[t])\n",
        "            total_predictions += 1\n",
        "          # Compute and print the loss for monitoring\n",
        "          loss = self.cross_entropy([cache[-1] for cache in caches], targets)\n",
        "          total_loss += loss\n",
        "          if batch_idx == rand_print:\n",
        "            print(\"Input sentence:\")\n",
        "            print([idx_to_word[i] for i in inputs])\n",
        "            print(\"Predicted:\")\n",
        "            print([idx_to_word[prediction] for prediction in predictions])\n",
        "            print(\"Amount of correct predictions\")\n",
        "            print(correct_predictions)\n",
        "            accuracy = correct_predictions / total_predictions\n",
        "            print(\"Accuracy -> \", accuracy)\n",
        "            print(\"Loss -> \", loss)\n",
        "        # Print epoch statistics\n",
        "        epoch_loss = total_loss / len(x_train)\n",
        "        epoch_accuracy = correct_predictions / total_predictions\n",
        "        print(f'Epoch {epoch}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}')"
      ],
      "metadata": {
        "id": "hEurHdYUUVz2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Training process</h2>"
      ],
      "metadata": {
        "id": "hu3yTJktUlyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 80\n",
        "\n",
        "vocabulary_size = len(vocabulary)\n",
        "hidden_size = 500\n",
        "lstm = LSTM(hidden_size=hidden_size, vocab_size=vocabulary_size,learning_rate=0.001)\n",
        "lstm.train(x_train,y_train,epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qfvGVr0YUsf9",
        "outputId": "737db02c-52a7-4e32-a346-e1978b131cc3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside train\n",
            "8\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'business', 'ap', 'assets', 'of', 'the', \"nation's\", 'retail', 'money', 'market', 'mutual', 'funds', 'fell', 'by', 'billion', 'in', 'the', 'latest', 'week', 'to', 'trillion', 'the', 'investment', 'company', 'institute', 'said', 'thursday']\n",
            "Predicted:\n",
            "['soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.012903225806451613\n",
            "Loss ->  [368.6584034]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 0, Loss: [511.92256362], Accuracy: 0.007142857142857143\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'sci', 'tech', 'reuters', 'a', 'group', 'of', 'technology', 'companies', 'including', 'texas', 'instruments', 'inc', 'txn', 'n', 'stmicroelectronics', 'stm', 'pa', 'and', 'broadcom', 'corp', 'brcm', 'o', 'on', 'thursday', 'said', 'they', 'will', 'propose', 'a', 'new', 'wireless', 'networking', 'standard', 'up', 'to', 'times', 'the', 'speed', 'of', 'the', 'current', 'generation']\n",
            "Predicted:\n",
            "['near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near', 'near']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.008130081300813009\n",
            "Loss ->  [571.56013193]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 1, Loss: [511.87224183], Accuracy: 0.007142857142857143\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'sci', 'tech', 'reuters', 'was', 'absenteeism', 'a', 'little', 'high', 'on', 'tuesday', 'among', 'the', 'guys', 'at', 'the', 'office', 'ea', 'sports', 'would', 'like', 'to', 'think', 'it', 'was', 'because', 'madden', 'nfl', 'came', 'out', 'that', 'day', 'and', 'some', 'fans', 'of', 'the', 'football', 'simulation', 'are', 'rabid', 'enough', 'to', 'take', 'a', 'sick', 'day', 'to', 'play', 'it']\n",
            "Predicted:\n",
            "['soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.00980392156862745\n",
            "Loss ->  [676.54929427]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 2, Loss: [511.83725214], Accuracy: 0.007142857142857143\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'business', 'reuters', 'stocks', 'ended', 'slightly', 'higher', 'on', 'friday', 'but', 'stayed', 'near', 'lows', 'for', 'the', 'year', 'as', 'oil', 'prices', 'surged', 'past', 'a', 'barrel', 'offsetting', 'a', 'positive', 'outlook', 'from', 'computer', 'maker', 'dell', 'inc', 'dell', 'o']\n",
            "Predicted:\n",
            "['soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.015503875968992248\n",
            "Loss ->  [451.40077923]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 3, Loss: [511.81295125], Accuracy: 0.007142857142857143\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'business', 'reuters', 'soaring', 'crude', 'prices', 'plus', 'worries', 'about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to', 'hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the', 'summer', 'doldrums']\n",
            "Predicted:\n",
            "['soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.030303030303030304\n",
            "Loss ->  [509.33611236]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 4, Loss: [511.79673357], Accuracy: 0.010714285714285714\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "Input sentence:\n",
            "['none', 'business', 'reuters', 'soaring', 'crude', 'prices', 'plus', 'worries', 'about', 'the', 'economy', 'and', 'the', 'outlook', 'for', 'earnings', 'are', 'expected', 'to', 'hang', 'over', 'the', 'stock', 'market', 'next', 'week', 'during', 'the', 'depth', 'of', 'the', 'summer', 'doldrums']\n",
            "Predicted:\n",
            "['soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring', 'soaring']\n",
            "Amount of correct predictions\n",
            "2\n",
            "Accuracy ->  0.030303030303030304\n",
            "Loss ->  [509.39609903]\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 12, 112, 17, 85, 32, 197, 82, 73, 174, 115, 151, 192, 87, 13, 120, 210, 193, 14, 41, 88, 37, 39, 89, 85, 124, 43, 199, 48]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 34, 181, 202, 104, 199, 100, 213, 105, 26, 167, 62, 73, 106, 2, 85, 7, 185, 168, 193, 139, 36, 193, 136, 141, 82, 30, 63, 22, 58, 22, 140]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 77, 173, 135, 73, 80, 52, 123, 187, 149, 111, 177, 122, 198, 151, 73, 146, 211, 191, 90, 73, 102, 121, 91, 43, 200]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194, 194]\n",
            "targets\n",
            "[67, 60, 182, 42, 53, 193, 78, 38, 199, 116, 15, 73, 145, 148, 73, 147, 138, 159, 84, 33, 191, 201, 209, 42, 93, 153, 160, 178, 44, 109, 8, 194, 125, 118, 135, 73, 196, 76, 117, 179, 189, 191, 86, 193, 207, 8, 191, 68, 209]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26]\n",
            "targets\n",
            "[67, 60, 182, 193, 74, 135, 205, 208, 130, 97, 214, 58, 142, 54, 128, 162, 23, 194, 133, 50, 71, 140, 199, 200, 43, 40, 24, 203, 193, 6, 65, 72, 186, 126, 191, 45, 73, 157, 135, 73, 127, 75]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[67, 60, 182, 81, 164, 199, 200, 43, 209, 134, 191, 129, 193, 131, 107, 98, 158, 131, 61, 194, 132, 46, 55, 184, 191, 4, 126, 62, 193, 106, 135, 180, 169, 212]\n",
            "vocabulary length\n",
            "216\n",
            "Epoch 5, Loss: [511.78699655], Accuracy: 0.010714285714285714\n",
            "Inside epoch\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "targets\n",
            "[5, 182, 49, 102, 31, 119, 74, 206, 10, 193, 95, 62, 103, 176, 59, 194, 154, 11, 195, 151, 73, 0, 152, 10, 47, 20, 70, 57, 199, 113, 143, 135, 73, 187]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n",
            "lengths --\n",
            "predictions\n",
            "[215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215]\n",
            "targets\n",
            "[5, 182, 215, 1, 7, 18, 92, 25, 73, 66, 194, 73, 141, 62, 144, 117, 114, 191, 101, 99, 73, 165, 187, 96, 211, 21, 73, 163, 135, 73, 204, 155]\n",
            "vocabulary length\n",
            "216\n",
            "inside batch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0161ec369769>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-05fc040675c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, epochs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m           \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m           \u001b[0mdWf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m           \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-05fc040675c8>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, caches, targets)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0;31m# Backpropagation through time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mdh_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWf_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWi_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWo_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbf_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbi_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbo_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbc_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWy_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_step_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mdWf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdWf_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdWi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdWi_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-05fc040675c8>\u001b[0m in \u001b[0;36mlstm_step_backward\u001b[0;34m(self, dh_next, dc_next, cache, target)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Gradients with respect to the weights and biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mdWf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mdWi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdWo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}